{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dataloader\n",
    "import imageio\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads frame sets that are hard to predict\n",
    "# used to quickly validate model training and progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video frames HARD!\n",
    "# 4 examples from the RedBullVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HARD_TEST_INSTANCES = {\n",
    "    'input_videos/redbull480.mp4': [116, 763, 1089, 1253]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_by_caption(cap, index):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return frame\n",
    "    \n",
    "    \n",
    "\n",
    "def generate_hard_input(quadratic=False):\n",
    "    \n",
    "    \n",
    "    if quadratic:\n",
    "        l = [-2, -1, 1, 2]\n",
    "    else:\n",
    "        l = [-1, 1]\n",
    "    \n",
    "    \n",
    "    for filename, frame_indices in HARD_TEST_INSTANCES.items():\n",
    "        \n",
    "#         video, _, _ = torchvision.io.read_video(filename)\n",
    "\n",
    "        cap = cv2.VideoCapture(filename)\n",
    "    \n",
    "        \n",
    "        for index in frame_indices:\n",
    "            frame_indices = [index+i for i in l]\n",
    "            X = [get_frame_by_caption(cap, i) for i in frame_indices]\n",
    "            X = torch.from_numpy(np.array(X))\n",
    "            X = X.permute(0,3,1,2).float()\n",
    "            print('shape', X.shape)\n",
    "            y = get_frame_by_caption(cap, index)\n",
    "            y = torch.from_numpy(y).permute(2,0,1).float()\n",
    "#             X = video[frame_indices].permute(0,3,1,2).float()\n",
    "            X = X.unsqueeze(dim=0)\n",
    "            print('shape2', X.shape, y.shape)\n",
    "            X = X.unbind(dim=0)\n",
    "            \n",
    "            yield X, y\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([2, 3, 480, 720])\n",
      "shape2 torch.Size([1, 2, 3, 480, 720]) torch.Size([3, 480, 720])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "  \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]),),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = generate_hard_input()\n",
    "\n",
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(y_hat, y, **args):\n",
    "    '''\n",
    "    Creates image grid by stacking predictions y_hat and \n",
    "    ground truth y.\n",
    "    \n",
    "    y_hat: predicted inputs, shape B x 3 x H x W\n",
    "    y: true values, same shape as y_hat\n",
    "    '''\n",
    "    \n",
    "    assert y_hat.shape == y.shape\n",
    "    \n",
    "    inp_tensor = torch.cat([y_hat, y])\n",
    "    grid = torchvision.utils.make_grid(inp_tensor, **args)\n",
    "    \n",
    "    return grid\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_hard_images(model):\n",
    "    gen = generate_hard_input(quadratic=False)\n",
    "\n",
    "    y_hats = []\n",
    "    ys = []\n",
    "    for (x1, x2), y in gen:\n",
    "        x1 = x1.cuda() / 255.\n",
    "        x2 = x2.cuda() / 255.\n",
    "        y = y / 255.\n",
    "        \n",
    "        y_hat = G(x1, x2).detach().cpu().squeeze(dim=0)\n",
    "        y_hats.append(y_hat)\n",
    "        ys.append(y)\n",
    "\n",
    "\n",
    "    y_hats = torch.stack(y_hats).clamp(0,1)\n",
    "    ys = torch.stack(ys)\n",
    "    \n",
    "    # create image grid\n",
    "    grid = create_grid(y_hats, ys, padding=20, nrow=4)\n",
    "    \n",
    "    return grid\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid = evalute_hard_images(G)\n",
    "\n",
    "plt.imshow(grid.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:17<00:00,  9.70s/it]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(names):\n",
    "    # init summarywriter\n",
    "    writer = SummaryWriter(f'runs/{name}')\n",
    "\n",
    "    # load model\n",
    "    G = torch.load(f'models/generator_{name}')\n",
    "    G = G.cuda()\n",
    "\n",
    "    # eval hard input\n",
    "    gen = generate_hard_input(quadratic=False)\n",
    "\n",
    "    y_hats = []\n",
    "    ys = []\n",
    "    for (x1, x2), y in gen:\n",
    "        x1 = x1.cuda() / 255.\n",
    "        x2 = x2.cuda() / 255.\n",
    "        y = y / 255.\n",
    "        \n",
    "        y_hat = G(x1, x2).detach().cpu().squeeze(dim=0)\n",
    "        y_hats.append(y_hat)\n",
    "        ys.append(y)\n",
    "\n",
    "\n",
    "    y_hats = torch.stack(y_hats).clamp(0,1)\n",
    "    ys = torch.stack(ys)\n",
    "    \n",
    "\n",
    "    # create image grid\n",
    "    grid = create_grid(y_hats, ys, padding=20, nrow=4)\n",
    "\n",
    "    writer.add_image('hard_examples', img_tensor=grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dataloader.adobe240_dataset()\n",
    "\n",
    "n_train = int(len(ds) * 0.8)\n",
    "n_valid = len(ds) - n_train\n",
    "\n",
    "\n",
    "_, valid = torch.utils.data.random_split(ds, [n_train, n_valid])\n",
    "dl = torch.utils.data.DataLoader(valid, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = os.listdir('runs')\n",
    "names = names[1:]\n",
    "\n",
    "filepaths = [f'models/generator_{name}' for name in names]\n",
    "filepaths.append('models/benchmark_generator_sepconv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/benchmark_generator_sepconv: 100%|██████████████████████████████████████████| 1198/1198 [10:29<00:00,  1.90it/s]\n",
      "models/generator_1586764929_0.0001_1e-05_False: 100%|██████████████████████████████| 1198/1198 [10:26<00:00,  1.91it/s]\n",
      "models/generator_1586716841_0.0001_1e-05_True: 100%|███████████████████████████████| 1198/1198 [10:27<00:00,  1.91it/s]\n",
      "models/generator_1586697759_0.0001_0_False: 100%|██████████████████████████████████| 1198/1198 [10:27<00:00,  1.91it/s]\n",
      "models/generator_1586688260_0.0001_0_True: 100%|███████████████████████████████████| 1198/1198 [10:26<00:00,  1.91it/s]\n",
      "models/generator_1586675975_1e-05_1e-05_False: 100%|███████████████████████████████| 1198/1198 [10:29<00:00,  1.90it/s]\n",
      "models/generator_1586606508_1e-05_1e-05_True: 100%|████████████████████████████████| 1198/1198 [10:27<00:00,  1.91it/s]\n",
      "models/generator_1586588130_1e-05_0_False: 100%|███████████████████████████████████| 1198/1198 [10:25<00:00,  1.91it/s]\n",
      "models/generator_1586529004_1e-05_0_True: 100%|████████████████████████████████████| 1198/1198 [10:26<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_path in reversed(filepaths):\n",
    "    # init summarywriter\n",
    "    \n",
    "    # load model\n",
    "    G = torch.load(model_path)\n",
    "    G = G.cuda()\n",
    "    G = G.eval()\n",
    "    \n",
    "    psnrs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (x1, x2), y in tqdm(dl, total=len(dl), desc=model_path):\n",
    "            x1 = x1.permute(0,3,1,2).cuda() / 255\n",
    "            x2 = x2.permute(0,3,1,2).cuda() / 255\n",
    "            y = y.permute(0,3,1,2).cuda()\n",
    "\n",
    "            y_hat = G(x1, x2).mul(255).clamp(0,255).int()\n",
    "\n",
    "            psnr = metrics.psnr(y_hat, y)\n",
    "            psnrs.extend(psnr)\n",
    "            \n",
    "    results[model_path] = psnrs\n",
    "        \n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvthesis",
   "language": "python",
   "name": "venvthesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
