{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code.sepconvfull import model\n",
    "import dataloader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import discriminator\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 0,\n",
    "    'amsgrad':False,\n",
    "    'loss': 'normal', # or wasserstein,\n",
    "    'input_size': 2 # or 4\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS=10\n",
    "writer = SummaryWriter('runs/adverserial_cond_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_weights(weights):\n",
    "    w = OrderedDict()\n",
    "    for key in weights:\n",
    "        new_key = 'get_kernel.'+key\n",
    "        w[new_key] = weights[key]\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init interpolation model\n",
    "sepconv = model.SepConvNet(kernel_size=51)\n",
    "\n",
    "weights = torch.load('code/sepconv/network-l1.pytorch')\n",
    "weights = convert_weights(weights)\n",
    "\n",
    "sepconv.load_state_dict(weights)\n",
    "opt = torch.optim.Adam(sepconv.parameters())\n",
    "\n",
    "# init discriminator\n",
    "disc_model = discriminator.Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepconv = sepconv.cuda()\n",
    "D = disc_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tensorboard_train(writer, metrics):\n",
    "    writer.add_scalar('Loss/train', np.mean(train_loss[epoch]), epoch)\n",
    "    writer.add_scalar('PSNR/train', np.mean(train_psnr[epoch]), epoch)\n",
    "    writer.add_scalar('IE/train', np.mean(train_ie[epoch]), epoch)\n",
    "    writer.add_scalar('Accuracy/train', np.mean(train_correct[epoch]), epoch)\n",
    "    \n",
    "    writer.add_histogram('Loss/train_hist', np.array(train_loss[epoch]), epoch)\n",
    "    writer.add_histogram('PSNR/train_hist', np.array(train_psnr[epoch]), epoch)\n",
    "    \n",
    "def write_tensorboard_valid(writer, metrics):\n",
    "    writer.add_scalar('Loss/valid', np.mean(valid_loss[epoch]), epoch)\n",
    "    writer.add_scalar('PSNR/valid', np.mean(valid_psnr[epoch]), epoch)\n",
    "    writer.add_scalar('IE/valid', np.mean(valid_ie[epoch]), epoch)\n",
    "    writer.add_scalar('Accuracy/valid', np.mean(valid_correct[epoch]), epoch)\n",
    "    \n",
    "    writer.add_histogram('Loss/valid_hist', np.array(valid_loss[epoch]), epoch)\n",
    "    writer.add_histogram('PSNR/valid_hist', np.array(valid_psnr[epoch]), epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultStore:\n",
    "    \n",
    "    def __init__(self, folds=['train', 'valid'], metrics=['psnr', 'ie', 'loss', 'accuracy'], writer=None):\n",
    "        self.folds = folds\n",
    "        self.metrics = metrics\n",
    "        self.results = dict()\n",
    "        self.writer = writer\n",
    "        \n",
    "        for fold in self.folds:\n",
    "            self.results[fold] = dict()\n",
    "            for metric in self.metrics:\n",
    "                self.results[fold][metric] = defaultdict(list)\n",
    "        \n",
    "    def store(self, fold, metric, epoch, value):\n",
    "        self.results[fold][metric][epoch].extend(value)\n",
    "        \n",
    "    def write_tensorboard(self, fold, epoch):\n",
    "        for metric in self.metrics:\n",
    "#             print(self.results[fold][metric][epoch])\n",
    "            mean = np.mean(self.results[fold][metric][epoch])\n",
    "            \n",
    "            self.writer.add_scalar(f'{metric}/{fold}', mean, epoch)\n",
    "            self.writer.add_histogram(f'{metric}/{fold}_hist', np.array(self.results[fold][metric][epoch]), epoch)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9941, 0.9966, 0.9948, 0.9981], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.ssim(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/10:   0%|                                                                           | 5/4826 [00:02<33:10,  2.42it/s]\n",
      "2/10:   0%|                                                                           | 5/4826 [00:02<32:50,  2.45it/s]\n",
      "3/10:   0%|                                                                           | 5/4826 [00:02<32:12,  2.49it/s]\n",
      "4/10:   0%|                                                                           | 5/4826 [00:02<32:40,  2.46it/s]\n",
      "5/10:   0%|                                                                           | 5/4826 [00:02<32:34,  2.47it/s]\n",
      "6/10:   0%|                                                                           | 5/4826 [00:02<33:21,  2.41it/s]\n",
      "7/10:   0%|                                                                           | 5/4826 [00:02<33:46,  2.38it/s]\n",
      "8/10:   0%|                                                                           | 5/4826 [00:02<32:47,  2.45it/s]\n",
      "9/10:   0%|                                                                           | 5/4826 [00:02<33:42,  2.38it/s]\n",
      "10/10:   0%|                                                                          | 5/4826 [00:02<33:35,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = dataloader.adobe240_dataset()\n",
    "ds = dataloader.TransformedDataset(ds, crop_size=(128,128))\n",
    "\n",
    "N_train = int(len(ds) * 0.8)\n",
    "N_valid = len(ds)-N_train\n",
    "\n",
    "train, valid = torch.utils.data.random_split(ds, [N_train, N_valid])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True, pin_memory=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid, batch_size=4, pin_memory=True)\n",
    "\n",
    "optimizer_G = torch.optim.Adam(sepconv.parameters(), lr=params['lr'], weight_decay=params['weight_decay'], amsgrad=params['amsgrad'])\n",
    "optimizer_D = torch.optim.Adam(D.parameters(), lr=params['lr'], weight_decay=params['weight_decay'], amsgrad=params['amsgrad'])\n",
    "critereon = torch.nn.L1Loss()\n",
    "\n",
    "# metrics\n",
    "# train_loss = defaultdict(list)\n",
    "# valid_loss = defaultdict(list)\n",
    "# train_psnr = defaultdict(list)\n",
    "# valid_psnr = defaultdict(list)\n",
    "# train_ie = defaultdict(list)\n",
    "# valid_ie = defaultdict(list)\n",
    "# train_correct = defaultdict(list)\n",
    "# valid_correct = defaultdict(list)\n",
    "R = ResultStore(writer=writer)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    sepconv.train()\n",
    "    D.train()\n",
    "    for i, ((x1, x2), y) in enumerate(tqdm(train_dl, total=len(train_dl), desc=f'{epoch+1}/{N_EPOCHS}')):\n",
    "        x1 = x1.cuda() / 255.\n",
    "        x2 = x2.cuda() / 255.\n",
    "        y = y.cuda() / 255.\n",
    "        \n",
    "        \n",
    "        y_hat = sepconv(x1, x2)\n",
    "        \n",
    "        l1_loss = critereon(y_hat, y)\n",
    "        \n",
    "\n",
    "        \n",
    "        loss = l1_loss - D(x1, x2, y_hat).sigmoid().mean()\n",
    "        \n",
    "        R.store('train', 'loss', epoch, [loss.item()])\n",
    "        \n",
    "        # compute psnr\n",
    "        y_hat = (y_hat * 255).clamp(0,255)\n",
    "        y = (y * 255).clamp(0,255)\n",
    "        \n",
    "        psnr = metrics.psnr(y_hat, y)\n",
    "        psnr = psnr.detach().cpu().tolist()\n",
    "#         train_psnr[epoch].extend(psnr)\n",
    "        R.store('train', 'psnr', epoch, psnr)\n",
    "        \n",
    "        ie = metrics.interpolation_error(y_hat, y)\n",
    "        ie = ie.detach().cpu().tolist()\n",
    "#         train_ie[epoch].extend(ie)\n",
    "        R.store('train', 'ie', epoch, ie)\n",
    "        \n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        l1_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # train discriminator\n",
    "        y_hat = y_hat.detach()\n",
    "        \n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        D_loss = D(x1, x2, y_hat).sigmoid().mean() - D(x1, x2, y).sigmoid().mean()\n",
    "        \n",
    "        correct_preds = (D(x1, x2, y_hat).sigmoid().round() == 0).flatten().int().detach().cpu().tolist()\n",
    "        correct_preds.extend((D(x1, x2, y).sigmoid().round() == 1).flatten().int().detach().cpu().tolist())\n",
    "#         train_correct[epoch].extend(correct_preds)\n",
    "        R.store('train', 'accuracy', epoch, correct_preds)\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        if i == 5:\n",
    "            break\n",
    "        \n",
    "    # update tensorboard\n",
    "    R.write_tensorboard('train', epoch)\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    sepconv.eval()\n",
    "    D.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, ((x1, x2), y) in enumerate(valid_dl):\n",
    "            x1 = x1.cuda() / 255.\n",
    "            x2 = x2.cuda() / 255.\n",
    "            y = y.cuda() / 255.\n",
    "\n",
    "            y_hat = sepconv(x1, x2)        \n",
    "            l1_loss = critereon(y_hat, y)        \n",
    "            loss = l1_loss - D(x1, x2, y_hat).sigmoid().mean()\n",
    "\n",
    "            R.store('valid', 'loss', epoch, [loss.item()])\n",
    "\n",
    "            \n",
    "            # compute psnr\n",
    "            y_hat = (y_hat * 255).clamp(0,255)\n",
    "            y = (y * 255).clamp(0,255)\n",
    "\n",
    "            psnr = metrics.psnr(y_hat, y)\n",
    "            psnr = psnr.detach().cpu().tolist()\n",
    "            R.store('valid', 'psnr', epoch, psnr)\n",
    "            \n",
    "            ie = metrics.interpolation_error(y_hat, y)\n",
    "            ie = ie.detach().cpu().tolist()\n",
    "            R.store('valid', 'ie', epoch, ie)\n",
    "\n",
    "            \n",
    "            y_hat = y_hat.detach()\n",
    "        \n",
    "            D_loss = D(x1, x2, y_hat).sigmoid().mean() - D(x1, x2, y).sigmoid().mean()\n",
    "\n",
    "            correct_preds = (D(x1, x2, y_hat).sigmoid().round() == 0).flatten().int().detach().cpu().tolist()\n",
    "            correct_preds.extend((D(x1, x2, y).sigmoid().round() == 1).flatten().int().detach().cpu().tolist())\n",
    "            R.store('valid', 'accuracy', epoch, correct_preds)\n",
    "            \n",
    "            if i == 5:\n",
    "                break\n",
    "            \n",
    "    # update tensorboard\n",
    "    R.write_tensorboard('valid', epoch)\n",
    "        \n",
    "# save models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_image('output', y_hat[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_model(y_hat).sigmoid().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D(y_hat).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dl):\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        for i, ((x1, x2), y) in enumerate(dl):\n",
    "            x1 = x1.permute(0,3,1,2).cuda() / 255.\n",
    "            x2 = x2.permute(0,3,1,2).cuda() / 255.\n",
    "            y = y.permute(0,3,1,2).cuda()\n",
    "\n",
    "            y_hat = model(x1, x2)\n",
    "            \n",
    "            y_hat = (y_hat * 255).clamp(0,255)\n",
    "\n",
    "            psnr = metrics.psnr(y_hat, y)\n",
    "            ie = metrics.interpolation_error(y_hat, y)\n",
    "            \n",
    "            psnr = psnr.detach().cpu().tolist()\n",
    "            ie = ie.detach().cpu().tolist()\n",
    "\n",
    "            results['psnr'].extend(psnr)\n",
    "            results['ie'].extend(ie)\n",
    "        \n",
    "    return results\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# t = dataloader2.Transformer(random_crop=False)\n",
    "ds = dataloader.adobe240_dataset(transformer=None)\n",
    "\n",
    "N_train = int(len(ds) * 0.8)\n",
    "N_test = len(ds)-N_train\n",
    "\n",
    "# _, test = torch.utils.data.random_split(ds, [N_train, N_test])\n",
    "\n",
    "test_dl = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=False)\n",
    "m = evaluate_model(sepconv, test_dl)\n",
    "\n",
    "np.mean(m['psnr']), np.mean(m['ie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataloader2.adobe240_dataset()\n",
    "# dataset = dataloader2.TransformedDataset(dataset, crop_size=(512, 512), h_flip_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = int(len(dataset) * 0.8)\n",
    "N_valid = int(len(dataset) * 0.1)\n",
    "N_test = len(dataset)-N_train-N_valid\n",
    "\n",
    "train, valid, test = torch.utils.data.random_split(dataset, [N_train, N_valid, N_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataloader2.TransformedDataset(train, crop_size=(512, 512), h_flip_prob=1)\n",
    "valid = dataloader2.TransformedDataset(valid, crop_size=(512, 512), h_flip_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train)\n",
    "valid_dl = torch.utils.data.DataLoader(valid)\n",
    "test_dl = torch.utils.data.DataLoader(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
