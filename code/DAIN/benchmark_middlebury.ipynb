{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy\n",
    "import networks\n",
    "# from my_args import  args\n",
    "# from scipy.misc import imread, imsave\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "\n",
    "from AverageMeter import  *\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from skimage.measure import compare_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.filter_size = 4\n",
    "        self.time_step = 0.5\n",
    "        self.channels = 3\n",
    "        self.netName = 'DAIN'\n",
    "        self.use_cuda = True\n",
    "        \n",
    "        self.save_which = 1\n",
    "        self.dtype = torch.cuda.FloatTensor\n",
    "        \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = networks.__dict__[args.netName](    channel=args.channels,\n",
    "                                    filter_size = args.filter_size ,\n",
    "                                    timestep=args.time_step,\n",
    "                                    training=False)\n",
    "\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAIN(\n",
       "  (initScaleNets_filter): ModuleList(\n",
       "    (0): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (12): ReLU()\n",
       "    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU()\n",
       "    (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU()\n",
       "    (19): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (20): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU()\n",
       "    (22): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (23): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU()\n",
       "    (25): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (26): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU()\n",
       "    (28): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (29): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (30): ReLU()\n",
       "    (31): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (32): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU()\n",
       "  )\n",
       "  (initScaleNets_filter1): ModuleList(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (initScaleNets_filter2): ModuleList(\n",
       "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (ctxNet): S2DF(\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (block3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (rectifyNet): MultipleBasicBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): Conv2d(437, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (block3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (block4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (block5): Sequential(\n",
       "      (0): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (flownets): PWCDCNet(\n",
       "    (conv1a): Sequential(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv1aa): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv1b): Sequential(\n",
       "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2a): Sequential(\n",
       "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2aa): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2b): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3a): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3aa): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3b): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4a): Sequential(\n",
       "      (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4aa): Sequential(\n",
       "      (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4b): Sequential(\n",
       "      (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5a): Sequential(\n",
       "      (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5aa): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5b): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6aa): Sequential(\n",
       "      (0): Conv2d(128, 196, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6a): Sequential(\n",
       "      (0): Conv2d(196, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6b): Sequential(\n",
       "      (0): Conv2d(196, 196, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (corr): Correlation()\n",
       "    (leakyRELU): LeakyReLU(negative_slope=0.1)\n",
       "    (conv6_0): Sequential(\n",
       "      (0): Conv2d(81, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6_1): Sequential(\n",
       "      (0): Conv2d(209, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6_2): Sequential(\n",
       "      (0): Conv2d(337, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6_3): Sequential(\n",
       "      (0): Conv2d(433, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv6_4): Sequential(\n",
       "      (0): Conv2d(497, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (predict_flow6): Conv2d(529, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (deconv6): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upfeat6): ConvTranspose2d(529, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv5_0): Sequential(\n",
       "      (0): Conv2d(213, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5_1): Sequential(\n",
       "      (0): Conv2d(341, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5_2): Sequential(\n",
       "      (0): Conv2d(469, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5_3): Sequential(\n",
       "      (0): Conv2d(565, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv5_4): Sequential(\n",
       "      (0): Conv2d(629, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (predict_flow5): Conv2d(661, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (deconv5): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upfeat5): ConvTranspose2d(661, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv4_0): Sequential(\n",
       "      (0): Conv2d(181, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4_1): Sequential(\n",
       "      (0): Conv2d(309, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4_2): Sequential(\n",
       "      (0): Conv2d(437, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4_3): Sequential(\n",
       "      (0): Conv2d(533, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv4_4): Sequential(\n",
       "      (0): Conv2d(597, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (predict_flow4): Conv2d(629, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (deconv4): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upfeat4): ConvTranspose2d(629, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv3_0): Sequential(\n",
       "      (0): Conv2d(149, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3_1): Sequential(\n",
       "      (0): Conv2d(277, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3_2): Sequential(\n",
       "      (0): Conv2d(405, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3_3): Sequential(\n",
       "      (0): Conv2d(501, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv3_4): Sequential(\n",
       "      (0): Conv2d(565, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (predict_flow3): Conv2d(597, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (deconv3): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (upfeat3): ConvTranspose2d(597, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2_0): Sequential(\n",
       "      (0): Conv2d(117, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2_1): Sequential(\n",
       "      (0): Conv2d(245, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2_2): Sequential(\n",
       "      (0): Conv2d(373, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2_3): Sequential(\n",
       "      (0): Conv2d(469, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (conv2_4): Sequential(\n",
       "      (0): Conv2d(533, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (predict_flow2): Conv2d(565, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (deconv2): ConvTranspose2d(2, 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (dc_conv1): Sequential(\n",
       "      (0): Conv2d(565, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (dc_conv2): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (dc_conv3): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (dc_conv4): Sequential(\n",
       "      (0): Conv2d(128, 96, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (dc_conv5): Sequential(\n",
       "      (0): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (dc_conv6): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.1)\n",
       "    )\n",
       "    (dc_conv7): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (depthNet): Sequential(\n",
       "    (0): Conv2d(3, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Sequential(\n",
       "      (0): LambdaMap(\n",
       "        (0): Sequential(\n",
       "          (0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (1): LambdaReduce(\n",
       "            (0): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (2): LambdaReduce(\n",
       "            (0): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (3): Sequential(\n",
       "            (0): LambdaMap(\n",
       "              (0): Sequential(\n",
       "                (0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "                (1): LambdaReduce(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (3): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (2): LambdaReduce(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (3): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (3): Sequential(\n",
       "                  (0): LambdaMap(\n",
       "                    (0): Sequential(\n",
       "                      (0): LambdaReduce(\n",
       "                        (0): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                        )\n",
       "                        (1): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (2): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (3): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                      )\n",
       "                      (1): LambdaReduce(\n",
       "                        (0): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                        )\n",
       "                        (1): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (2): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (3): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(64, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (1): Sequential(\n",
       "                      (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "                      (1): LambdaReduce(\n",
       "                        (0): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                        )\n",
       "                        (1): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (2): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (3): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                      )\n",
       "                      (2): LambdaReduce(\n",
       "                        (0): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                        )\n",
       "                        (1): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (2): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (3): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                      )\n",
       "                      (3): Sequential(\n",
       "                        (0): LambdaMap(\n",
       "                          (0): Sequential(\n",
       "                            (0): LambdaReduce(\n",
       "                              (0): Sequential(\n",
       "                                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                              )\n",
       "                              (1): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (2): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (3): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                            )\n",
       "                            (1): LambdaReduce(\n",
       "                              (0): Sequential(\n",
       "                                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                              )\n",
       "                              (1): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (2): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (3): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                            )\n",
       "                          )\n",
       "                          (1): Sequential(\n",
       "                            (0): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
       "                            (1): LambdaReduce(\n",
       "                              (0): Sequential(\n",
       "                                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                              )\n",
       "                              (1): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (2): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (3): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                            )\n",
       "                            (2): LambdaReduce(\n",
       "                              (0): Sequential(\n",
       "                                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                              )\n",
       "                              (1): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (2): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (3): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                            )\n",
       "                            (3): LambdaReduce(\n",
       "                              (0): Sequential(\n",
       "                                (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                              )\n",
       "                              (1): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (2): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                              (3): Sequential(\n",
       "                                (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                                (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (2): ReLU()\n",
       "                                (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                                (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                                (5): ReLU()\n",
       "                              )\n",
       "                            )\n",
       "                            (4): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
       "                          )\n",
       "                        )\n",
       "                        (1): LambdaReduce()\n",
       "                      )\n",
       "                      (4): LambdaReduce(\n",
       "                        (0): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                        )\n",
       "                        (1): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (2): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (3): Sequential(\n",
       "                          (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                      )\n",
       "                      (5): LambdaReduce(\n",
       "                        (0): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                        )\n",
       "                        (1): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (2): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(64, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                        (3): Sequential(\n",
       "                          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (2): ReLU()\n",
       "                          (3): Conv2d(64, 64, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "                          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                          (5): ReLU()\n",
       "                        )\n",
       "                      )\n",
       "                      (6): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
       "                    )\n",
       "                  )\n",
       "                  (1): LambdaReduce()\n",
       "                )\n",
       "                (4): LambdaReduce(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (3): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (5): LambdaReduce(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (3): Sequential(\n",
       "                    (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (6): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): LambdaReduce(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (3): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                )\n",
       "                (1): LambdaReduce(\n",
       "                  (0): Sequential(\n",
       "                    (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                  )\n",
       "                  (1): Sequential(\n",
       "                    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (2): Sequential(\n",
       "                    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                  (3): Sequential(\n",
       "                    (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (2): ReLU()\n",
       "                    (3): Conv2d(64, 32, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "                    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "                    (5): ReLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): LambdaReduce()\n",
       "          )\n",
       "          (4): LambdaReduce(\n",
       "            (0): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(64, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "              (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (5): LambdaReduce(\n",
       "            (0): Sequential(\n",
       "              (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(32, 16, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "          )\n",
       "          (6): UpsamplingNearest2d(scale_factor=2.0, mode=nearest)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LambdaReduce(\n",
       "            (0): Sequential(\n",
       "              (0): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "            )\n",
       "            (1): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (2): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(64, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "            (3): Sequential(\n",
       "              (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (2): ReLU()\n",
       "              (3): Conv2d(64, 16, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n",
       "              (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n",
       "              (5): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): LambdaReduce()\n",
       "    )\n",
       "    (4): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CUDA\n"
     ]
    }
   ],
   "source": [
    "if args.use_cuda:\n",
    "    print('USING CUDA')\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.SAVED_MODEL = '../../models/dain/best.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing model weight is: ../../models/dain/best.pth\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(args.SAVED_MODEL):\n",
    "    print(\"The testing model weight is: \" + args.SAVED_MODEL)\n",
    "    if not args.use_cuda:\n",
    "        pretrained_dict = torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage)\n",
    "        # model.load_state_dict(torch.load(args.SAVED_MODEL, map_location=lambda storage, loc: storage))\n",
    "    else:\n",
    "        pretrained_dict = torch.load(args.SAVED_MODEL)\n",
    "        # model.load_state_dict(torch.load(args.SAVED_MODEL))\n",
    "\n",
    "    model_dict = model.state_dict()\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    # 4. release the pretrained dict for saving memory\n",
    "    pretrained_dict = []\n",
    "else:\n",
    "    print(\"*****************************************************************\")\n",
    "    print(\"**** We don't load any trained weights **************************\")\n",
    "    print(\"*****************************************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda=args.use_cuda\n",
    "save_which=args.save_which\n",
    "dtype = args.dtype\n",
    "unique_id =str(random.randint(0, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding(intWidth, intHeight, channel):\n",
    "    \n",
    "    if intWidth != ((intWidth >> 7) << 7):\n",
    "        intWidth_pad = (((intWidth >> 7) + 1) << 7)  # more than necessary\n",
    "        intPaddingLeft =int(( intWidth_pad - intWidth)/2)\n",
    "        intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n",
    "    else:\n",
    "        intWidth_pad = intWidth\n",
    "        intPaddingLeft = 32\n",
    "        intPaddingRight= 32\n",
    "\n",
    "    if intHeight != ((intHeight >> 7) << 7):\n",
    "        intHeight_pad = (((intHeight >> 7) + 1) << 7)  # more than necessary\n",
    "        intPaddingTop = int((intHeight_pad - intHeight) / 2)\n",
    "        intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n",
    "    else:\n",
    "        intHeight_pad = intHeight\n",
    "        intPaddingTop = 32\n",
    "        intPaddingBottom = 32\n",
    "    \n",
    "    \n",
    "    return intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_image(im):\n",
    "    X = torch.from_numpy( np.transpose(im, (2,0,1)).astype('float32')/255.0).type(dtype)\n",
    "    \n",
    "    intWidth = X.size(2)\n",
    "    intHeight = X.size(1)\n",
    "    channel = X.size(0)\n",
    "    \n",
    "    \n",
    "    intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom = get_padding(intWidth, intHeight, channel)\n",
    "    \n",
    "    pader = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom])\n",
    "    \n",
    "    torch.set_grad_enabled(False)\n",
    "    X = Variable(torch.unsqueeze(X,0))\n",
    "    X = pader(X)\n",
    "    \n",
    "    return X, (intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def interpolate(im1, im2):\n",
    "    X0, pad_0 = preprocess_image(im1)\n",
    "    X1, pad_1 = preprocess_image(im2)\n",
    "    \n",
    "    y_ = torch.FloatTensor()\n",
    "\n",
    "    assert pad_0 == pad_1\n",
    "    \n",
    "    intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom = pad_0\n",
    "    \n",
    "    if use_cuda:\n",
    "        X0 = X0.cuda()\n",
    "        X1 = X1.cuda()\n",
    "        \n",
    "    y_s,offset,filter = model(torch.stack((X0, X1),dim = 0))\n",
    "    y_ = y_s[save_which]\n",
    "    \n",
    "    \n",
    "    if use_cuda:\n",
    "        X0 = X0.data.cpu().numpy()\n",
    "        if not isinstance(y_, list):\n",
    "            y_ = y_.data.cpu().numpy()\n",
    "        else:\n",
    "            y_ = [item.data.cpu().numpy() for item in y_]\n",
    "        offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n",
    "        filter = [filter_i.data.cpu().numpy() for filter_i in filter]  if filter[0] is not None else None\n",
    "        X1 = X1.data.cpu().numpy()\n",
    "    else:\n",
    "        X0 = X0.data.numpy()\n",
    "        if not isinstance(y_, list):\n",
    "            y_ = y_.data.numpy()\n",
    "        else:\n",
    "            y_ = [item.data.numpy() for item in y_]\n",
    "        offset = [offset_i.data.numpy() for offset_i in offset]\n",
    "        filter = [filter_i.data.numpy() for filter_i in filter]\n",
    "        X1 = X1.data.numpy()\n",
    "        \n",
    "    X0 = np.transpose(255.0 * X0.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n",
    "\n",
    "\n",
    "    y_ = [np.transpose(255.0 * item.clip(0,1.0)[:, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for item in y_]\n",
    "\n",
    "\n",
    "    offset = [np.transpose(offset_i[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for offset_i in offset]\n",
    "    filter = [np.transpose(\n",
    "        filter_i[0, :, intPaddingTop:intPaddingTop + intHeight, intPaddingLeft: intPaddingLeft + intWidth],\n",
    "        (1, 2, 0)) for filter_i in filter]  if filter is not None else None\n",
    "    X1 = np.transpose(255.0 * X1.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n",
    "    \n",
    "    return y_[0]\n",
    "    \n",
    "    \n",
    "def interpolate_old(im1, im2):\n",
    "    X0 = torch.from_numpy( np.transpose(im1, (2,0,1)).astype('float32')/255.0).type(dtype)\n",
    "    X1 = torch.from_numpy( np.transpose(im2, (2,0,1)).astype('float32')/255.0).type(dtype)\n",
    "\n",
    "    y_ = torch.FloatTensor()\n",
    "\n",
    "    assert (X0.size(1) == X1.size(1))\n",
    "    assert (X0.size(2) == X1.size(2))\n",
    "    \n",
    "    intWidth = X0.size(2)\n",
    "    intHeight = X0.size(1)\n",
    "    channel = X0.size(0)\n",
    "\n",
    "    if intWidth != ((intWidth >> 7) << 7):\n",
    "        intWidth_pad = (((intWidth >> 7) + 1) << 7)  # more than necessary\n",
    "        intPaddingLeft =int(( intWidth_pad - intWidth)/2)\n",
    "        intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n",
    "    else:\n",
    "        intWidth_pad = intWidth\n",
    "        intPaddingLeft = 32\n",
    "        intPaddingRight= 32\n",
    "\n",
    "    if intHeight != ((intHeight >> 7) << 7):\n",
    "        intHeight_pad = (((intHeight >> 7) + 1) << 7)  # more than necessary\n",
    "        intPaddingTop = int((intHeight_pad - intHeight) / 2)\n",
    "        intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n",
    "    else:\n",
    "        intHeight_pad = intHeight\n",
    "        intPaddingTop = 32\n",
    "        intPaddingBottom = 32\n",
    "\n",
    "    pader = torch.nn.ReplicationPad2d([intPaddingLeft, intPaddingRight , intPaddingTop, intPaddingBottom])\n",
    "    \n",
    "    torch.set_grad_enabled(False)\n",
    "    X0 = Variable(torch.unsqueeze(X0,0))\n",
    "    X1 = Variable(torch.unsqueeze(X1,0))\n",
    "    X0 = pader(X0)\n",
    "    X1 = pader(X1)\n",
    "    \n",
    "    \n",
    "    if use_cuda:\n",
    "        X0 = X0.cuda()\n",
    "        X1 = X1.cuda()\n",
    "    model.eval()\n",
    "    y_s,offset,filter = model(torch.stack((X0, X1),dim = 0))\n",
    "    y_ = y_s[save_which]\n",
    "    \n",
    "    \n",
    "    if use_cuda:\n",
    "        X0 = X0.data.cpu().numpy()\n",
    "        if not isinstance(y_, list):\n",
    "            y_ = y_.data.cpu().numpy()\n",
    "        else:\n",
    "            y_ = [item.data.cpu().numpy() for item in y_]\n",
    "        offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n",
    "        filter = [filter_i.data.cpu().numpy() for filter_i in filter]  if filter[0] is not None else None\n",
    "        X1 = X1.data.cpu().numpy()\n",
    "    else:\n",
    "        X0 = X0.data.numpy()\n",
    "        if not isinstance(y_, list):\n",
    "            y_ = y_.data.numpy()\n",
    "        else:\n",
    "            y_ = [item.data.numpy() for item in y_]\n",
    "        offset = [offset_i.data.numpy() for offset_i in offset]\n",
    "        filter = [filter_i.data.numpy() for filter_i in filter]\n",
    "        X1 = X1.data.numpy()\n",
    "        \n",
    "    X0 = np.transpose(255.0 * X0.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n",
    "\n",
    "\n",
    "    y_ = [np.transpose(255.0 * item.clip(0,1.0)[:, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for item in y_]\n",
    "    # 0 weggehaald als index\n",
    "\n",
    "    offset = [np.transpose(offset_i[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0)) for offset_i in offset]\n",
    "    filter = [np.transpose(\n",
    "        filter_i[0, :, intPaddingTop:intPaddingTop + intHeight, intPaddingLeft: intPaddingLeft + intWidth],\n",
    "        (1, 2, 0)) for filter_i in filter]  if filter is not None else None\n",
    "    X1 = np.transpose(255.0 * X1.clip(0,1.0)[0, :, intPaddingTop:intPaddingTop+intHeight, intPaddingLeft: intPaddingLeft+intWidth], (1, 2, 0))\n",
    "    del X0; del X1\n",
    "    return y_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(y_hat, y):\n",
    "    R = np.max([y_hat, y])\n",
    "    return 10 * np.log10(R**2 / mse(y_hat, y))\n",
    "\n",
    "def mse(y_hat, y):\n",
    "    return np.mean((y_hat-y)**2)\n",
    "\n",
    "def ssim(y_hat, y):\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolated.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x221456d2930>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]c:\\users\\ruth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "c:\\users\\ruth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py:2693: UserWarning: Default grid_sample and affine_grid behavior will be changed to align_corners=False from 1.4.0. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\"Default grid_sample and affine_grid behavior will be changed \"\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "  8%|██████▉                                                                            | 1/12 [00:01<00:13,  1.22s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 17%|█████████████▊                                                                     | 2/12 [00:02<00:11,  1.17s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 25%|████████████████████▊                                                              | 3/12 [00:03<00:10,  1.19s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 33%|███████████████████████████▋                                                       | 4/12 [00:04<00:09,  1.20s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 42%|██████████████████████████████████▌                                                | 5/12 [00:05<00:08,  1.20s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 50%|█████████████████████████████████████████▌                                         | 6/12 [00:07<00:07,  1.17s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 58%|████████████████████████████████████████████████▍                                  | 7/12 [00:08<00:05,  1.18s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 8/12 [00:09<00:04,  1.14s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 9/12 [00:10<00:03,  1.16s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 10/12 [00:11<00:02,  1.17s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      " 92%|███████████████████████████████████████████████████████████████████████████▏      | 11/12 [00:12<00:01,  1.03s/it]..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "..\\torch\\csrc\\autograd\\python_function.cpp:622: UserWarning: Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:13<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "psnrs = []\n",
    "ssims = []\n",
    "ies   = []\n",
    "for folder in tqdm(os.listdir('MiddleBurySet/other-data')):\n",
    "    image1 = imread(os.path.join('MiddleBurySet/other-data', folder, 'frame10.png'))\n",
    "    image2 = imread(os.path.join('MiddleBurySet/other-data', folder, 'frame11.png'))\n",
    "    imagei = imread(os.path.join('MiddleBurySet/other-gt-interp', folder, 'frame10i11.png'))\n",
    "    \n",
    "    imagei = imagei.astype('float32')\n",
    "    \n",
    "    interpolated= interpolate_old(image1, image2)\n",
    "    \n",
    "    psnr_score = psnr(interpolated, imagei)\n",
    "    ie_score = np.sqrt(mse(interpolated, imagei))\n",
    "    ssim_score = compare_ssim(interpolated, imagei, multichannel=True)\n",
    "    psnrs.append(psnr_score)\n",
    "    ssims.append(ssim_score)\n",
    "    ies.append(ie_score)\n",
    "#     print(psnr_score, ssim_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.43683571482669, 0.8642908167888198, 4.1719284)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(psnrs), np.mean(ssims), np.mean(ies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.43683515128822, 0.8642908151116094)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(psnrs), np.mean(ssims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
